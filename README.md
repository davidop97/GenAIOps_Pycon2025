# ü§ñ Chatbot GenAI ‚Äì Caso de Estudio Recursos Humanos

Este proyecto ilustra c√≥mo construir, evaluar y desplegar un chatbot de tipo RAG (Retrieval Augmented Generation) con buenas pr√°cticas de GenAIOps, adaptado a un dominio de Ingenier√≠a de Datos y Ciencia de Datos en AWS. Aqu√≠ se documenta el flujo completo, los cambios realizados y los resultados obtenidos.

---

## üß† Resumen del Proyecto

* **Dominio**: Ingenier√≠a de Datos y Ciencia de Datos en AWS (en lugar de Recursos Humanos).

* **Lenguaje**: Python 3.13.1.

* **Frameworks / Librer√≠as**:

  * **LangChain** + **OpenAI** ‚Äì para RAG y evaluaci√≥n (QAEvalChain, LabeledCriteriaEvalChain).
  * **FAISS** ‚Äì como vectorstore local.
  * **Streamlit** ‚Äì interfaz web (chat + dashboard).
  * **MLflow** ‚Äì para registro de experimentos y m√©tricas.
  * **pytest** ‚Äì pruebas unitarias.

* **Estructura de prompts**:

  * `v1_machine_learning_engineer`
  * `v2_senior_machine_learning_engineer`

* **Configuraci√≥n de chunks**: `CHUNK_SIZE=1024` y `CHUNK_OVERLAP=150` (en `.env`).

* **Tests modificados**:

  * Se actualiz√≥ `tests/test_run_eval.py` para validar `correctness_score` en lugar de `lc_is_correct`.
  * Se eliminaron referencias a `eval_dataset.csv` (ya no existe).
  * Las preguntas de evaluaci√≥n en `tests/eval_dataset.json` fueron actualizadas al nuevo dominio.

* **Archivos eliminados**:

  * `tests/eval_dataset.csv` (solo queda `eval_dataset.json`).

* **Dashboard mejorado** (`app/dashboard.py`):

  * Visualiza m√©tricas por criterio (`correctness_score`, `relevance_score`, `coherence_score`, `toxicity_score`, `harmfulness_score`, `clarity_score`).
  * Permite comparar diferentes criterios y configuraciones (`prompt_version + chunk_size`).
  * Incluye opci√≥n para mostrar el ‚Äúrazonamiento‚Äù (artifact) de cada criterio para cada run.

### Resultados principales

* **Correctness Score promedio** de la configuraci√≥n actual (`PROMPT_VERSION=v2_senior_machine_learning_engineer` con chunks = 1024/150) qued√≥ en **0.3333 (33.33 %)**, es decir, solo **2 de 6 preguntas** se calificaron como ‚Äúcorrectas‚Äù.
* Esto indica que, aunque el prompt y los chunks grandes facilitan que la informaci√≥n relevante se capture en un solo vector, el modelo a√∫n falla en la mayor√≠a de las respuestas.
* **Mejor opci√≥n actual**: usar `1024 / 150` obtuvo mayor cobertura del contexto completo de los documentos (menos ‚Äúcortar‚Äù informaci√≥n √∫til), pero puede pagar con respuestas menos precisas si no se optimiza el prompt o la selecci√≥n de los fragments.

**Posibles mejoras**:

1. Probar chunks m√°s peque√±os (p.ej. 256/100 o 512/50) para aislar mejor cada secci√≥n y aumentar el likelihood de encontrar el pasaje exacto que responde la pregunta.
2. Afinar a√∫n m√°s el prompt (ej. `v3_*`), incorporando ejemplos de buena respuesta para reforzar `correctness`.
3. Aumentar el dataset de preguntas/respuestas de prueba para entrenar mejor el tono y asegurar ejemplos variados (hasta 10‚Äì12 preguntas).
4. Ajustar temperatura, top\_p o par√°metros de OpenAI si hay hallazgos de incoherencia o ‚Äúrazonamientos extra√±os‚Äù.
5. Explorar otros embedddings (p.ej. OpenAI‚Äôs `text-embedding-3-small`) o √≠ndices (p.ej. Pinecone/Weaviate) si FAISS local se queda corto.

---

## üìÇ Estructura del Proyecto

```
chatbot-genaiops/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ ui_streamlit.py                 ‚Üê Interfaz simple del chatbot
‚îÇ   ‚îú‚îÄ‚îÄ main_interface.py               ‚Üê Interfaz combinada con m√©tricas
‚îÇ   ‚îú‚îÄ‚îÄ dashboard.py                    ‚Üê Dashboard Streamlit mejorado
‚îÇ   ‚îú‚îÄ‚îÄ run_eval.py                     ‚Üê Script de evaluaci√≥n autom√°tica
‚îÇ   ‚îú‚îÄ‚îÄ rag_pipeline.py                 ‚Üê L√≥gica de ingesti√≥n y RAG pipeline
‚îÇ   ‚îî‚îÄ‚îÄ prompts/
‚îÇ       ‚îú‚îÄ‚îÄ v1_machine_learning_engineer.txt       ‚Üê Prompt versi√≥n 1
‚îÇ       ‚îî‚îÄ‚îÄ v2_senior_machine_learning_engineer.txt‚Üê Prompt versi√≥n 2 mejorado
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îî‚îÄ‚îÄ pdfs/                           ‚Üê Documentos fuente (PDFs de AWS/Data Eng)
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_run_eval.py                ‚Üê Prueba unitaria actualizada
‚îÇ   ‚îî‚îÄ‚îÄ eval_dataset.json               ‚Üê Dataset de evaluaci√≥n JSON
‚îú‚îÄ‚îÄ .env.example                        ‚Üê Ejemplo de variables de entorno
‚îú‚îÄ‚îÄ .env                                ‚Üê Variables locales (no comitear el real)
‚îú‚îÄ‚îÄ requirements.txt                    ‚Üê Dependencias (ajustadas para Python 3.13.1)
‚îú‚îÄ‚îÄ Dockerfile                          ‚Üê Para contenedor (ajustado si es necesario)
‚îú‚îÄ‚îÄ .devcontainer/                      ‚Üê Configuraci√≥n de devcontainer (VS Code)
‚îÇ   ‚îî‚îÄ‚îÄ devcontainer.json
‚îî‚îÄ‚îÄ .github/workflows/
    ‚îú‚îÄ‚îÄ eval.yml                       ‚Üê CI de evaluaci√≥n con pytest + MLflow
    ‚îî‚îÄ‚îÄ test.yml                       ‚Üê CI de pruebas unitarias con pytest

```

---

## üß± 1. Preparaci√≥n del Entorno

1. **Clonar el repositorio**:

   ```bash
   git clone https://github.com/davidop97/GenAIOps_Pycon2025.git
   cd chatbot-genaiops
   ```

2. **Crear entorno conda** (Python 3.13.1):

   ```bash
   conda create -n chatbot-genaiops python=3.13.1 -y
   conda activate chatbot-genaiops
   ```

3. **Instalar dependencias** (requirements actualizados):

   ```bash
   pip install -r requirements.txt
   ```

4. **Variables de entorno**:
   Duplique el ejemplo y edite `.env` (no hacer commit de tu llave).

   ```bash
   cp .env.example .env  
   ```

   Edite `.env` para reflejar la configuraci√≥n actual (ejemplo):

   ```dotenv
   # Clave de OpenAI
   OPENAI_API_KEY=tu_api_key_aqu√≠

   # RAG Configuration
   PROMPT_VERSION=v2_senior_machine_learning_engineer
   CHUNK_SIZE=1024
   CHUNK_OVERLAP=150
   ```

---

## üîç 2. Ingesta y Vectorizaci√≥n de Documentos

> **Atenci√≥n**: Antes de este paso, coloque todos los PDFs de AWS/Data Eng en `data/pdfs/`.

Ejecute para procesar los PDFs y generar el √≠ndice FAISS local:

```bash
python -c "from app.rag_pipeline import save_vectorstore; save_vectorstore()"
```

* **chunk\_size=1024** y **chunk\_overlap=150** se toman de las variables de entorno en `.env`.
* El resultado se guarda en `vectorstore/` (sobrescribe el anterior).
* Se crea un registro en MLflow bajo el experimento `vectorstore_tracking` con los par√°metros usados.

**Si desea cambiar la granularidad de los chunks**, edite `.env` y luego vuelva a correr el comando anterior. Por ejemplo, para probar con 512/50, ponga `CHUNK_SIZE=512` y `CHUNK_OVERLAP=50`, vuelva a generar.

---

## üß† 3. Construcci√≥n del Pipeline RAG

En Python (o dentro de `run_eval.py`, `ui_streamlit.py` o `main_interface.py`), cargue el vectorstore y cree la cadena con la versi√≥n de prompt actual:

```python
from app.rag_pipeline import load_vectorstore_from_disk, build_chain

vectordb = load_vectorstore_from_disk()  # Carga FAISS local
chain = build_chain(vectordb, prompt_version="v2_senior_machine_learning_engineer")
```

* El prompt se lee autom√°ticamente de `app/prompts/v2_senior_machine_learning_engineer.txt` en UTF-8.
* El `ConversationalRetrievalChain` usa internamente `ChatOpenAI(model="gpt-4o")` y el retriever sobre FAISS.

---

## üí¨ 4. Prueba Manual via Streamlit

1. **Interfaz b√°sica**:

   ```bash
   streamlit run app/ui_streamlit.py
   ```

   * Permite hacer preguntas al chatbot en una ventana web.
   * Usa el contexto de los PDFs y el prompt activo.
   * Muestra historial de conversaci√≥n.

2. **Interfaz con m√©tricas**:

   ```bash
   streamlit run app/main_interface.py
   ```

   * Igual que la b√°sica, pero incluye una pesta√±a ‚ÄúM√©tricas‚Äù que muestra `correctness_score` promedio (antes `lc_is_correct`).

3. **Dashboard completo**:

   ```bash
   streamlit run app/dashboard.py
   ```

   * Muestra tabla con cada pregunta evaluada y todos los `*_score` (correctness, relevance, coherence, toxicity, harmfulness, clarity).
   * Permite seleccionar un criterio y ver un gr√°fico de barras con promedio por `prompt_version | chunk_size`.
   * Ofrece desplegable para elegir un run y bot√≥n ‚ÄúMostrar reasoning‚Äù para ver el razonamiento (artifact) de ese criterio (archivo `reasons/<criterio>/reason_<criterio>.txt`).
   * Muestra estad√≠sticas generales agrupadas por configuraci√≥n y permite descargar CSV.

---

## üß™ 5. Evaluaci√≥n Autom√°tica de Calidad

1. **Dataset de evaluaci√≥n**:

   * `tests/eval_dataset.json` contiene ahora 6 pares (pregunta ‚Üî respuesta esperada) orientados a AWS/Data Engineering.
   * Ejemplo de entrada:

     ```json
     [
       {
         "question": "¬øC√≥mo ingiero datos en tiempo real en AWS?",
         "answer": "Para ingesta en tiempo real AWS recomienda ... Kinesis Data Streams ... DataSync"
       },
       ... (5 preguntas m√°s)
     ]
     ```

2. **Script de evaluaci√≥n** (`app/run_eval.py`):

   ```bash
   python app/run_eval.py
   ```

   * Por cada pregunta, genera la respuesta RAG (`gen`) y luego eval√∫a **6 criterios** con `LabeledCriteriaEvalChain` (uno a uno):

     1. correctness
     2. relevance
     3. coherence
     4. toxicity
     5. harmfulness
     6. clarity

   * Para cada criterio, se registra en MLflow:

     * `correctness_score`, `correctness_reason`
     * `relevance_score`, `relevance_reason`
     * ‚Ä¶
     * `clarity_score`, `clarity_reason`

   * Cada run es nombrado `eval_q<√≠ndice>`. Los artifacts (razonamientos) se almacenan en:

     ```
     mlruns/
       <id_experimento_eval_v2_senior_machine_learning_engineer>/
         <run_id>/
           artifacts/
             reasons/
               correctness/reason_correctness.txt
               relevance/reason_relevance.txt
               ...
               clarity/reason_clarity.txt
     ```

3. **Revisi√≥n de resultados en consola**:
   Al ejecutarlo deber√≠as ver algo as√≠ para cada pregunta:

   ```
   Pregunta 1/6: ¬øC√≥mo ingiero datos en tiempo real en AWS?
   Respuesta generada: ‚ÄúPara ingesta en tiempo real ...‚Äù
      ‚Ä¢ correctness: score=1, verdict=Y
      ‚Ä¢ relevance:   score=1, verdict=Y
      ‚Ä¢ coherence:   score=1, verdict=Y
      ‚Ä¢ toxicity:    score=1, verdict=Y
      ‚Ä¢ harmfulness: score=1, verdict=Y
      ‚Ä¢ clarity:     score=1, verdict=Y
   ```

4. **Registro en MLflow**:

   * Cada run guarda par√°metros (`question`, `prompt_version`, `chunk_size`, `chunk_overlap`) y m√©tricas (`*_score`).
   * Los razonamientos se suben como artifacts de texto.

---

## ‚úÖ 6. Validaci√≥n Autom√°tica con pytest

**Test actualizado**: `tests/test_run_eval.py` se modific√≥ para validar √∫nicamente `correctness_score` ‚â• 0.8 dentro de cada experimento.

```bash
pytest tests/test_run_eval.py
```

Contenido relevante de `test_run_eval.py`:

```python
import mlflow
import pytest

def test_relevancia_minima():
    client = mlflow.tracking.MlflowClient()
    experiments = [e for e in client.search_experiments() if e.name.startswith("eval_")]
    assert experiments, "No hay experimentos con nombre 'eval_'"

    for exp in experiments:
        runs = client.search_runs(experiment_ids=[exp.experiment_id])
        assert runs, f"No hay ejecuciones en el experimento {exp.name}"

        scores = [
            r.data.metrics.get("correctness_score", 0)
            for r in runs
            if "correctness_score" in r.data.metrics
        ]

        if scores:
            promedio = sum(scores) / len(scores)
            print(f"Precisi√≥n (correctness_score) promedio en {exp.name}: {promedio:.2f}")
            assert promedio >= 0.8, f"Precisi√≥n insuficiente en {exp.name}: {promedio:.2f}"
        else:
            pytest.fail(f"No se encontraron m√©tricas 'correctness_score' en {exp.name}")
```

> **Nota**: Dado que la ejecuci√≥n actual arroj√≥ un 0.33 (33 %) de `correctness_score` promedio en `eval_v2_senior_machine_learning_engineer`, este test fallar√°.

---

## üìà 7. Dashboard y Mejora de Visualizaci√≥n

El dashboard (`app/dashboard.py`) ofrece las siguientes secciones:

1. **Resultados individuales**

   * Tabla con cada run (`run_id`) y las m√©tricas: `correctness_score`, `relevance_score`, `coherence_score`, `toxicity_score`, `harmfulness_score`, `clarity_score`.

2. **Visualizaci√≥n por criterio**

   * Men√∫ desplegable con todos los criterios (`*_score`).
   * Gr√°fico de barras que muestra el promedio de dicho criterio por cada configuraci√≥n (`prompt_version` + `chunk_size`).

3. **Razonamiento del modelo**

   * Seleccionar un `run_id` y un criterio para descargar el artifact `reason_<criterio>.txt` y mostrarlo en pantalla.

4. **Estad√≠sticas generales**

   * Tabla con el promedio de todos los criterios agregados por `prompt_version` y `chunk_size`.
   * Bot√≥n para descargar CSV con esas estad√≠sticas.

Estas visualizaciones permiten r√°pidamente observar:

* **Qu√© configuraciones produjeron mejores `correctness_score`** (idealmente ‚â• 0.8).
* Si un modelo falla un criterio espec√≠fico (p. ej. baja `coherence_score`), se puede revisar el razonamiento para entender el porqu√©.

---

## üîß 8. Ajustes y Posibles Mejoras

1. **Granularidad de chunks**

   * Actualmente se usa `CHUNK_SIZE=1024`, `CHUNK_OVERLAP=150` para maximizar la inclusi√≥n de contexto en un mismo chunk. Esto aumenta la probabilidad de que el fragmento que contiene la respuesta quede completo en un solo embeding.
   * Sin embargo, el resultado de `correctness_score=0.33` indica que el modelo a√∫n no encuentra la informaci√≥n exacta en 4 de cada 6 casos.
   * **Prueba alternativa**:

     * `CHUNK_SIZE=512`, `CHUNK_OVERLAP=100`
     * `CHUNK_SIZE=256`, `CHUNK_OVERLAP=100`
     * Al crear chunks m√°s peque√±os con solapamiento alto, a veces es m√°s f√°cil que el sistema recupere fragmentos de texto muy focalizados que contengan la respuesta literal.

2. **Ajuste de prompt**
   
   * El prompt `v2_senior_machine_learning_engineer` enfatiza correcci√≥n y estructura, pero podr√≠a no ser suficiente si los PDFs tienen frases muy largas o vocabulario distinto.
   * **Sugerencia**:

     * Crear un `v3_machine_learning_engineer_examples` que incluya 1‚Äì2 ejemplos de preguntas y respuestas bien respondidas (few-shot), para reforzar `correctness`.
     * Incluir un recordatorio en el prompt de ‚Äúbusca coincidencias literales en el contexto‚Äù si existe una frase id√©ntica en el PDF.

3. **Dataset de preguntas**

   * Solo hay actualmente 6 preguntas (33 % correctness).
   * **Aumentar a 10‚Äì12 preguntas** de distintos niveles de complejidad, para tener un test set m√°s robusto y poder identificar patrones en las fallas.

4. **Par√°metros de LLM**

   * Ajustar `temperature=0` (ya est√°), `max_tokens`, o incluso usar `top_p=0.9` para ver si baja la probabilidad de respuestas ‚Äúinventadas‚Äù.
   * Probar `model="gpt-4o"` vs. `model="gpt-3.5-turbo"` para comparar costos vs. calidad.

5. **√çndice vectorial**

   * FAISS local es r√°pido, pero no permite b√∫squedas sem√°nticas avanzadas (p.ej. ANN con fines de cero-shot). Como pr√≥ximo paso se podr√≠a migrar a un servicio gestionado (Pinecone, Weaviate, Qdrant).

6. **Evaluaci√≥n multicit√©rio**

   * Actualmente solo validamos `correctness_score`.
   * En una segunda fase, habilitar tests para `relevance_score ‚â• 0.8` y `coherence_score ‚â• 0.8`, garantizando que las respuestas sean no solo correctas, sino tambi√©n relevantes y bien estructuradas.

---

## üìú Resumen Final

* Se migr√≥ el dominio a Ingenier√≠a de Datos / AWS.
* Se crearon **dos versiones de prompt** (`v1_machine_learning_engineer` y `v2_senior_machine_learning_engineer`).
* Se actualiz√≥ la configuraci√≥n de chunks a **1024/150**.
* Se modific√≥ `tests/test_run_eval.py` para validar `correctness_score` ‚â• 0.8 por experimento.
* Se elimin√≥ `eval_dataset.csv`; ahora solo se usa `eval_dataset.json`.
* Se mejor√≥ el dashboard para mostrar todas las m√©tricas y los razonamientos de cada criterio.
* El accuracy actual de `correctness` es **33.33 %**, indicando la necesidad de iterar en prompt y chunks.
* Se proponen varias mejoras a futuro (chunks m√°s peque√±os, prompt con ejemplos few-shot, dataset m√°s grande) para elevar el rendimiento del chatbot.


