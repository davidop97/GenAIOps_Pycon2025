[
  {
    "question": "¿Cómo ingiero datos en tiempo real en AWS?",
    "answer": "Para ingesta en tiempo real AWS recomienda servicios como Amazon Kinesis Data Streams (captura de datos de streaming), Kinesis Data Firehose (entrega automática en S3, Redshift o Elasticsearch) y Amazon MSK (Kafka gestionado). También puedes usar AWS IoT Core para telemetría de dispositivos y AWS DataSync para transferencias continuas."
  },
  {
    "question": "¿Qué servicio serverless de AWS debo usar para procesos ETL?",
    "answer": "AWS Glue es el servicio ETL serverless recomendado: utiliza Glue Crawlers para descubrir esquemas, el Glue Data Catalog para metadatos y trabajos en Python/Scala para transformar y cargar datos directamente en S3, Redshift o bases de datos compatibles."
  },
  {
    "question": "¿Cómo construyo un Data Lake seguro en AWS?",
    "answer": "Un Data Lake seguro se construye sobre Amazon S3 con AWS Lake Formation para gestionar permisos a nivel de tabla y fila, cifrado en reposo (SSE-S3 o SSE-KMS) y en tránsito (TLS), IAM para control de acceso, y AWS CloudTrail/CloudWatch para auditoría y monitoreo."
  },
  {
    "question": "¿Qué opciones de consulta interactiva de datos ofrece AWS?",
    "answer": "AWS ofrece Amazon Athena para consultas SQL serverless directamente sobre datos en S3, Redshift Spectrum para federar consultas entre Amazon Redshift y S3, y Amazon QuickSight para dashboards interactivos basados en esos resultados."
  },
  {
    "question": "¿Cómo orquesto un pipeline de Machine Learning en AWS?",
    "answer": "Para ML en producción AWS sugiere usar Amazon SageMaker Pipelines junto con AWS Step Functions: define etapas de preparación (Processing), entrenamiento (Training), evaluación (Model Evaluation) y despliegue (Model Registry → Endpoint), todo orquestado de forma automatizada y reproducible."
  },
  {
    "question": "¿Qué buenas prácticas recomienda AWS para optimizar costos en un Data Lake?",
    "answer": "Entre las prácticas clave están usar políticas de ciclo de vida en S3 (mover datos antiguos a S3 Glacier), comprimir y particionar datos, habilitar el versionado con limpieza automática, elegir el formato columnar (Parquet/ORC) y aprovisionar instancias spot o escalado automático en servicios como EMR y Glue."
  }
]
