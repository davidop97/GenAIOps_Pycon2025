[
  {
    "question": "Cuales son los servicios recomendados para la ingesta de datos en tiempo real en AWS segun los documentos internos?",
    "answer": "Para la ingesta de datos en tiempo real, AWS recomienda Amazon Kinesis Data Streams para capturar datos de streaming, Kinesis Data Firehose para entrega automatica a S3 o Redshift, y Amazon MSK como una opcion de Kafka gestionado. Tambien se puede usar AWS IoT Core para datos de dispositivos."
  },
  {
    "question": "Que servicio serverless de AWS es recomendado para procesos ETL segun los documentos internos?",
    "answer": "AWS Glue es el servicio serverless recomendado para procesos ETL, con Glue Crawlers para descubrir esquemas, Glue Data Catalog para gestionar metadatos y trabajos en Python o Scala para transformar y cargar datos en S3 o Redshift."
  },
  {
    "question": "Cuales son las mejores practicas para construir un Data Lake seguro en AWS segun los documentos internos?",
    "answer": "Las mejores practicas para construir un Data Lake seguro en AWS incluyen usar Amazon S3 con cifrado SSE-KMS, implementar AWS Lake Formation para gestionar permisos, configurar IAM para control de acceso y habilitar auditoria con AWS CloudTrail."
  },
  {
    "question": "Que servicios de AWS son recomendados para consultas interactivas de datos segun los documentos internos?",
    "answer": "AWS recomienda Amazon Athena para consultas SQL serverless en S3, Amazon Redshift Spectrum para consultas federadas entre Redshift y S3, y AWS Glue Data Catalog para gestionar metadatos que soportan consultas interactivas."
  },
  {
    "question": "Cuales son los pasos para construir un pipeline de Machine Learning listo para produccion en AWS segun los documentos internos?",
    "answer": "Para construir un pipeline de Machine Learning listo para produccion en AWS, los pasos incluyen usar Amazon SageMaker para entrenamiento y despliegue de modelos, AWS Step Functions para orquestar el flujo de trabajo, AWS Lambda para procesamiento basado en eventos y Amazon CloudWatch para monitoreo."
  },
  {
    "question": "Cuales son las practicas recomendadas para optimizar costos en un Data Lake en AWS segun los documentos internos?",
    "answer": "Las practicas recomendadas para optimizar costos en un Data Lake en AWS incluyen usar politicas de ciclo de vida de S3 para mover datos a S3 Glacier, comprimir datos en formatos como Parquet, particionar datos y monitorear costos con AWS Cost Explorer."
  }
]